from calibre.ptempfile import PersistentTemporaryFile
from calibre.web.feeds.news import BasicNewsRecipe
from calibre.ebooks.BeautifulSoup import BeautifulSoup
import re
import urllib

#class BioinfoRecipe(BasicNewsRecipe):
	
	#keepArticles = [re.compile('')]
	
	
	# this is the parser for bioinformatics journals in the form of pure urls.
	# assume that different recipes will change the keepArticles to be a different regex


class OxfordRecipe(BasicNewsRecipe):
  
	title = u'Oxford Journals'
	__author__ = 'Robert M Flight'
	language = 'en'
	#articles_are_obfuscated = True
	
	remove_tags_before = dict(attrs={'class':'article fulltext-view'})
	remove_tags_after = dict(attrs={'id':'related-urls'})
	
	keepArticles = [re.compile('oxfordjournals')]
	
	match_regexps = [r'expansion.html']
	recursions = 2
	
	# this does the parsing of the text file with the URLs. Please change this to your own hosted text file
	def parse_index(self):
		feeds = []
		for title, url in [('Oxford Articles', 'https://dl.dropboxusercontent.com/s/ytcduwmj1kn335g/test_oxford.txt')]:
			articles = self.bioinfo_parse_articles(url)
			if articles:
				feeds.append((title, articles))
			print feeds
		return feeds
		
	def bioinfo_parse_articles(self,url):
		urlFile = urllib.urlopen(url)
		useURL = urlFile.readlines()
		current_articles = []

		incTitle = 1

		for urlI in useURL:
			if any(r.search(urlI) for r in self.keepArticles):
				splitURL = urlI.split("|")
				if len(splitURL) == 1:
					current_articles.append({'title': "Article " + str(incTitle), 'url': splitURL[0].strip(), 'description':'', 'date':''})
				else:
					current_articles.append({'title': splitURL[0].strip(), 'url':splitURL[1].strip(), 'description':'', 'date':''})
				incTitle = incTitle + 1
		return current_articles
		
	def preprocess_html(self, soup):
		
		allImg = soup.findAll('img')
		
		for imgTag in allImg:
			tmpSrc = imgTag['src']
			if re.search('.small.gif', tmpSrc):
				tmpSrc = re.sub('.small.gif', '.large.jpg', tmpSrc)
				imgTag['width'] = 600
				
			imgTag['src'] = tmpSrc
		
		return soup
		
	def postprocess_html(self, soup, first_fetch):
		url_split_char = '/'
		beg_link = "links"
		
		all_tables = soup.findAll('div', 'table pos-float')
		for iTable in all_tables:
			a_link = iTable.find('a', 'in-nw')
			
			print "this is the table link: " + a_link['href']
			
			
			table_file = open(a_link['href'], 'r').read()
			table_soup = BeautifulSoup(table_file)
			
			table_div = table_soup.find('div', 'table-expansion ')
			
			table_href = a_link['href']
			split_ref = table_href.split(url_split_char)
			for i in xrange(2):
				use_loc = split_ref.pop(-1)
				
			#print "This is the url I want: " + use_loc
			
			table_img = table_soup.findAll('img', 'inline-graphic')
			for iImg in table_img:
				full_link = url_split_char.join([beg_link, use_loc, iImg['src']])
				iImg['src'] = full_link
				print "Here are table graphics:" + iImg['src']
			
			iTable.replaceWith(table_div)
		
		return soup
