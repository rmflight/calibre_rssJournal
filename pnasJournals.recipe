from calibre.ptempfile import PersistentTemporaryFile
from calibre.web.feeds.news import BasicNewsRecipe
from calibre.ebooks.BeautifulSoup import BeautifulSoup
import re
import urllib

# the goal with this recipe (and others I'm writing) is that a reader of 
# academic journals frequently has a list of articles to read, from a variety of journals.
# Based on this workflow, this recipe assumes a single file with the titles and urls of 
# the articles in question, separated by the pipe |
#
# eg Article of interest|url.of.the.article
#
# This file is set in "articleFile" below
#
# PNAS articles are defined by their URLs, if one is missing you may need to add it

# this recipe assumes that there is a text file hosted somewhere (articleFile)
# where each line is either a url, or a title|url pair (title and url separated by a pipe)
# These are the articles you actually want to read.

class PNASRecipe(BasicNewsRecipe):
  
	title = u'PNAS'
	__author__ = 'Robert M Flight'
	__date__ = '23 May 2013'
	__version__ = '1.0'
	language = 'en'
	__license__ = 'GPL v3'
	
	articleFile = 'http://rmflight.github.io/calibre_rssJournal/testJournals.txt'
	#articles_are_obfuscated = True
	
	#no_stylesheets = True
	#keep_only_tags = dict(attrs={'class':'article fulltext-view '})
	
	remove_tags_before = dict(attrs={'class':['article fulltext-view ', 'article fulltext-view', 'expansion-article-title']})
	remove_tags_after = dict(attrs={'class':'social-bookmarking'})	
		
	keepArticles = [re.compile('http://www.pnas.org/')]
	
	match_regexps = [r'expansion.html']
	recursions = 2
	
	# this does the parsing of the text file with the URLs. Please change this to your own hosted text file
	def parse_index(self):
		feeds = []
		for title, url in [('PNAS Articles', self.articleFile)]:
			articles = self.bioinfo_parse_articles(url)
			if articles:
				feeds.append((title, articles))
			print feeds
		return feeds
		
	def bioinfo_parse_articles(self,url):
		urlFile = urllib.urlopen(url)
		useURL = urlFile.readlines()
		current_articles = []
		

		incTitle = 1

		for urlI in useURL:
			if any(r.search(urlI) for r in self.keepArticles):
				splitURL = urlI.split("|")
				if len(splitURL) == 1:
					useURL = splitURL[0].strip()
					useTitle = "Article" + str(incTitle)
				else:
					useURL = splitURL[1].strip()
					useTitle = splitURL[0].strip()
				
				current_articles.append({'title': useTitle, 'url':useURL, 'description':'', 'date':''})
				incTitle = incTitle + 1
		return current_articles
		
	def preprocess_html(self, soup):
		
		# in contrast to other recipes, PLOS uses png images for both images and tables
		allFigures = soup.findAll('div', 'fig-inline')
		
		for figDiv in allFigures:
			imgTag = figDiv('img')[0]
			tmpSrc = imgTag['src']
			#tmpSrc = url_split_char.join([base_url, tmpSrc])
			if re.search('gif', tmpSrc):
				tmpSrc = re.sub('.small.gif', '.large.jpg', tmpSrc)
				
			imgTag['src'] = tmpSrc
		
		return soup
		
	def postprocess_html(self, soup, first_fetch):
		url_split_char = '/'
		beg_link = "links"
		
		all_tables = soup.findAll('div', 'table pos-float')
		for iTable in all_tables:
			
			a_link = iTable.find('a')
			
			print "this is the table link: " + a_link['href']
			
			
			table_file = open(a_link['href'], 'r').read()
			table_soup = BeautifulSoup(table_file)
			print repr(table_soup)
			table_div = []
			
			# this is needed because some tables use different classes
			try:
				table_div = table_soup.findAll('div', 'table-expansion')[0]
			except:
				print "no space didn't work!"
				
			try:
				table_div = table_soup.findAll('div', 'table-expansion ')[0]
			except:
				print "with space didn't work!"
			
			#print repr(table_div)
			
			table_href = a_link['href']
			split_ref = table_href.split(url_split_char)
			for i in xrange(2):
				use_loc = split_ref.pop(-1)
				
			#print "This is the url I want: " + use_loc
			
			table_img = table_soup.findAll('img', 'inline-graphic')
			for iImg in table_img:
				full_link = url_split_char.join([beg_link, use_loc, iImg['src']])
				iImg['src'] = full_link
				print "Here are table graphics:" + iImg['src']
			
			if len(table_div) != 0:
				iTable.replaceWith(table_div)
		
		return soup
